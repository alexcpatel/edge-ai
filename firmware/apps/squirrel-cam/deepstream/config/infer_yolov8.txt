# YOLOv8 Inference Configuration for DeepStream
# Uses TensorRT for GPU-accelerated inference on Jetson

[property]
gpu-id=0
net-scale-factor=0.0039215697906911373
# YOLOv8 input size
infer-dims=3;640;640
uff-input-blob-name=images
batch-size=1
# 0=FP32, 1=INT8, 2=FP16
network-mode=2
num-detected-classes=80
interval=0
gie-unique-id=1
# Model paths - engine will be generated on first run
model-engine-file=/models/yolov8n.engine
# ONNX model for initial conversion
onnx-file=/models/yolov8n.onnx
labelfile-path=/config/labels.txt
# Custom parser for YOLOv8 output
parse-bbox-func-name=NvDsInferParseYolo
custom-lib-path=/opt/nvidia/deepstream/deepstream/lib/libnvds_infercustomparser.so
# Output tensor meta for custom parsing
output-tensor-meta=1
# Clustering
cluster-mode=2
maintain-aspect-ratio=1
symmetric-padding=1

[class-attrs-all]
pre-cluster-threshold=0.25
nms-iou-threshold=0.45
topk=300
